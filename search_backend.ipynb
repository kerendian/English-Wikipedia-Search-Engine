{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/noa-blenkitny/IR_Project/blob/main/search_backend.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Authenticate your user\n",
        "# The authentication should be done with the email connected to your GCP account\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "6ehmyiEwmAs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy one wikidumps files \n",
        "import os\n",
        "from pathlib import Path\n",
        "from google.colab import auth\n",
        "\n",
        "project_id = 'core-period-321814'\n",
        "!gcloud config set project {project_id}\n",
        "\n",
        "data_bucket_name = 'wikidata_preprocessed'\n",
        "try:\n",
        "    if os.environ[\"wikidata_preprocessed\"] is not None:\n",
        "        pass  \n",
        "except:\n",
        "      !mkdir wikidumps\n",
        "      !gsutil cp gs://{data_bucket_name}/multistream1_preprocessed.parquet \"wikidumps/\" "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBwXXFQRRuvt",
        "outputId": "32fdee15-63ee-4785-c2c8-f68b9a2d0b09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [core/project].\n",
            "\u001b[1;33mWARNING:\u001b[0m You do not appear to have access to project [core-period-321814] or it does not exist.\n",
            "mkdir: cannot create directory ‘wikidumps’: File exists\n",
            "Copying gs://wikidata_preprocessed/multistream1_preprocessed.parquet...\n",
            "- [1 files][316.7 MiB/316.7 MiB]                                                \n",
            "Operation completed over 1 objects/316.7 MiB.                                    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pyspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt-get update -qq\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "!pip install -q graphframes\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "graphframes_jar = 'https://repos.spark-packages.org/graphframes/graphframes/0.8.2-spark3.2-s_2.12/graphframes-0.8.2-spark3.2-s_2.12.jar'\n",
        "spark_jars = '/usr/local/lib/python3.7/dist-packages/pyspark/jars'\n",
        "!wget -N -P $spark_jars $graphframes_jar\n",
        "import pyspark\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SQLContext\n",
        "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
        "from graphframes import *\n",
        "# Initializing spark context\n",
        "# create a spark context and session\n",
        "conf = SparkConf().set(\"spark.ui.port\", \"4050\")\n",
        "#sc.stop()\n",
        "sc = pyspark.SparkContext(conf=conf)\n",
        "sc.addPyFile(str(Path(spark_jars) / Path(graphframes_jar).name))\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxB2JOEOT8IM",
        "outputId": "915a5318-09bb-4256-d696-a21ec1dc8997"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk-8-jdk-headless is already the newest version (8u312-b07-0ubuntu1~18.04).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 100 not upgraded.\n",
            "--2022-01-02 14:58:53--  https://repos.spark-packages.org/graphframes/graphframes/0.8.2-spark3.2-s_2.12/graphframes-0.8.2-spark3.2-s_2.12.jar\n",
            "Resolving repos.spark-packages.org (repos.spark-packages.org)... 13.35.101.96, 13.35.101.23, 13.35.101.120, ...\n",
            "Connecting to repos.spark-packages.org (repos.spark-packages.org)|13.35.101.96|:443... connected.\n",
            "HTTP request sent, awaiting response... 304 Not Modified\n",
            "File ‘/usr/local/lib/python3.7/dist-packages/pyspark/jars/graphframes-0.8.2-spark3.2-s_2.12.jar’ not modified on server. Omitting download.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#when we move to gcp- nedd to change it to the entire wikipdia corpus\n",
        "from pathlib import Path \n",
        "import os\n",
        "\n",
        "try:\n",
        "    if os.environ[\"wikidata_preprocessed\"] is not None:\n",
        "      path = os.environ[\"wikidata_preprocessed\"]+\"/wikidumps/*\"\n",
        "except:\n",
        "      path = \"wikidumps/*\"\n",
        "\n",
        "parquetFile = spark.read.parquet(path)\n",
        "parquetFile.count()"
      ],
      "metadata": {
        "id": "lZTj9GZ2RvqZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "936d4fa6-ed46-4fa5-c555-497e83bdf28b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21084"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs_list = [23862,7089,21035]\n",
        "body_data = parquetFile.select(\"id\",\"text\").rdd.filter(lambda x: x[0] in docs_list)\n",
        "title_data = parquetFile.select(\"id\",\"title\").rdd.filter(lambda x: x[0] in docs_list)\n",
        "raw_anchor_data = parquetFile.select(\"id\",\"anchor_text\").rdd.filter(lambda x: x[0] in docs_list)\n",
        "#using set because eventually the anchor index will use binary ranking and we want unique tokens.\n",
        "#filtering only the text of anchor because we think that if a doc points to another in a specific topic the\n",
        "#doc is more imprtant. we will use it in the binary calculations of search score\n",
        "anchor_data = raw_anchor_data.mapValues(lambda x: ','.join(set(item[1] for item in x)))\n",
        "#print(body_data.take(3))\n",
        "#print(title_data.take(3))\n",
        "#print(anchor_data.take(1))"
      ],
      "metadata": {
        "id": "Xmh8QBqVR7Qx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.core.display import Math\n",
        "from inverted_index_colab import InvertedIndex\n",
        "from inverted_index_colab import MultiFileReader\n",
        "from inverted_index_colab import MultiFileWriter\n",
        "import math\n",
        "import numpy as np\n",
        "import builtins\n",
        "import nltk\n",
        "from nltk.stem.porter import *\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from collections import OrderedDict, Counter, defaultdict\n",
        "import hashlib\n",
        "def _hash(s):\n",
        "    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n",
        "\n",
        "nltk.download('stopwords')\n",
        "#using code from assignment 3- gcp\n",
        "#funcs to help build the inverted index\n",
        "english_stopwords = frozenset(stopwords.words('english'))\n",
        "corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\", \n",
        "                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\", \n",
        "                    \"part\", \"thumb\", \"including\", \"second\", \"following\", \n",
        "                    \"many\", \"however\", \"would\", \"became\"]\n",
        "\n",
        "all_stopwords = english_stopwords.union(corpus_stopwords)\n",
        "RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n",
        "\n",
        "NUM_BUCKETS = 124\n",
        "def token2bucket_id(token):\n",
        "  return int(_hash(token),16) % NUM_BUCKETS\n",
        "\n",
        "def calc_doc_len(text, id):\n",
        "  tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n",
        "  tokens = [token for token in tokens if token not in all_stopwords]\n",
        "  return len(tokens)\n",
        "\n",
        "def word_count(text, id):\n",
        "  \n",
        "  ''' Count the frequency of each word in `text` (tf) that is not included in \n",
        "  `all_stopwords` and return entries that will go into our posting lists. \n",
        "  Parameters:\n",
        "  -----------\n",
        "    text: str\n",
        "      Text of one document\n",
        "    id: int\n",
        "      Document id\n",
        "  Returns:\n",
        "  --------\n",
        "    List of tuples\n",
        "      A list of (token, (doc_id, tf)) pairs \n",
        "      for example: [(\"Anarchism\", (12, 5)), ...]\n",
        "  '''\n",
        "  tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n",
        "  tokens = [token for token in tokens if token not in all_stopwords ]\n",
        "  word_count_list = []\n",
        "  words_dict= OrderedDict()\n",
        "  for word in tokens:\n",
        "    words_dict[word] = words_dict.get(word,0) + 1\n",
        "  for item in words_dict.items():\n",
        "    word_count_list.append((item[0],(id,item[1])))\n",
        "  return word_count_list\n",
        "\n",
        "def reduce_word_counts(unsorted_pl):\n",
        "  ''' Returns a sorted posting list by wiki_id.\n",
        "  Parameters:\n",
        "  -----------\n",
        "    unsorted_pl: list of tuples\n",
        "      A list of (wiki_id, tf) tuples \n",
        "  Returns:\n",
        "  --------\n",
        "    list of tuples\n",
        "      A sorted posting list.\n",
        "  '''\n",
        "  return  sorted(unsorted_pl,key=lambda y: y[0])\n",
        "\n",
        "def calculate_df(postings):\n",
        "  ''' Takes a posting list RDD and calculate the df for each token.\n",
        "  Parameters:\n",
        "  -----------\n",
        "    postings: RDD\n",
        "      An RDD where each element is a (token, posting_list) pair.\n",
        "  Returns:\n",
        "  --------\n",
        "    RDD\n",
        "      An RDD where each element is a (token, df) pair.\n",
        "  '''\n",
        "\n",
        "  return postings.mapValues(lambda x : len(x))\n",
        "\n",
        "def partition_postings_and_write(postings,new_index_name):\n",
        "  ''' A function that partitions the posting lists into buckets, writes out \n",
        "  all posting lists in a bucket to disk, and returns the posting locations for \n",
        "  each bucket. Partitioning should be done through the use of `token2bucket` \n",
        "  above. Writing to disk should use the function  `write_a_posting_list`, a \n",
        "  static method implemented in inverted_index_colab.py under the InvertedIndex \n",
        "  class. \n",
        "  Parameters:\n",
        "  -----------\n",
        "    postings: RDD\n",
        "      An RDD where each item is a (w, posting_list) pair.\n",
        "  Returns:\n",
        "  --------\n",
        "    RDD\n",
        "      An RDD where each item is a posting locations dictionary for a bucket. The\n",
        "      posting locations maintain a list for each word of file locations and \n",
        "      offsets its posting list was written to. See `write_a_posting_list` for \n",
        "      more details.\n",
        "  '''\n",
        "  #partitioning RDD to (bucket_id, [(w0, posting_list_0)) entries \n",
        "  postings = postings.map(lambda x: (new_index_name+str(token2bucket_id(x[0])),x))\n",
        "  #reducing it to (bucket_id, [(w0, posting_list_0), (w1, posting_list_1), ...])\n",
        "  postings = postings.groupByKey()\n",
        "  #writing each bucket to disk\n",
        "  #need to change it in gcp to:\n",
        "  #postings = postings.map(lambda y: (InvertedIndex.write_a_posting_list(y,'3b1u8c3k6e4t122')))\n",
        "  #in colab:\n",
        "  postings = postings.map(lambda y: (InvertedIndex.write_a_posting_list(y)))\n",
        "  return postings\n",
        "  \n",
        "def calc_tf_idf(tf, N , df, doc_len):\n",
        "  tf_idf = (tf/doc_len)* math.log(N/df)\n",
        "  return tf_idf\n",
        "\n"
      ],
      "metadata": {
        "id": "hAcfNhveW_Gz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57f75a15-a503-4163-80b7-e937c628e291"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from time import time\n",
        "def create_inverted_index_with_tf_idf_weights(data,new_index_name):\n",
        "\n",
        "    # word counts map - posting lists\n",
        "    word_counts = data.flatMap(lambda x: word_count(x[1], x[0]))\n",
        "    #united posting lists\n",
        "    postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n",
        "    #RDD to map doc_id to doc_len\n",
        "    doc_lens = data.map(lambda x: (x[0], calc_doc_len(x[1], x[0])))\n",
        "    # turn the doc_lens RDD into dict - TODO: check how its gonna be for the whole corpus\n",
        "    doc_lens = doc_lens.collectAsMap()\n",
        "    # print(doc_lens)\n",
        "    # RDD to map {doc_id,[(token,tf),(token,tf)....]}\n",
        "    tf = postings.flatMap(lambda x: [(tup[0], (x[0],tup[1])) for tup in x[1]] ) \n",
        "    # groupBy doc_id\n",
        "    tf = tf.groupByKey().mapValues(list)\n",
        "   \n",
        "    # filtering postings and calculate df\n",
        "    #Next, we will filter out rare words, words that appear in 10 or fewer documents (when working on the entire corpus, we will increase this threshold to a minimum of 50 documents).\n",
        "    #postings_filtered = postings.filter(lambda x: len(x[1])>50) #why filter?\n",
        "    #w2df = calculate_df(postings_filtered)\n",
        "    \n",
        "    w2df = calculate_df(postings)\n",
        "    w2df_dict = w2df.collectAsMap()\n",
        "\n",
        "    # partition posting lists and write out- in gcp\n",
        "    #_ = partition_postings_and_write(postings_filtered).collect()\n",
        "    #_ = partition_postings_and_write(postings_filtered)\n",
        "\n",
        "    #in colab:\n",
        "    #write posting lists to disk and returns posting locs dict for each bucket\n",
        "    posting_locs_list = partition_postings_and_write(postings,new_index_name).collect()\n",
        "    \n",
        "\n",
        "    # collect all posting lists locations into one super-set- gcp code:\n",
        "    # super_posting_locs = defaultdict(list)\n",
        "    # for blob in client.list_blobs(bucket_name, prefix='postings_gcp'):\n",
        "    #   if not blob.name.endswith(\"pickle\"):\n",
        "    #     continue\n",
        "    #   with blob.open(\"rb\") as f:\n",
        "    #     posting_locs = pickle.load(f)\n",
        "    #     for k, v in posting_locs.items():\n",
        "    #       super_posting_locs[k].extend(v)\n",
        "\n",
        "    # merge the posting locations into a single dict- good for local test in colab\n",
        "    super_posting_locs = defaultdict(list)\n",
        "    for posting_loc in posting_locs_list:\n",
        "        for k, v in posting_loc.items():\n",
        "          super_posting_locs[k].extend(v)\n",
        "\n",
        "      #for k, v in posting_loc.items():\n",
        "        #super_posting_locs[k].extend(v)\n",
        "\n",
        "    new_index = InvertedIndex()\n",
        "    # Adding the posting locations dictionary to the inverted index\n",
        "    new_index.posting_locs = super_posting_locs\n",
        "    # Add the token - df dictionary to the inverted index\n",
        "    new_index.df = w2df_dict\n",
        "    # the number of the docs in the corpus - using to calc TF-IDF\n",
        "    new_index.N= data.count()\n",
        "    # print(new_index.N)\n",
        "    # tf = postings.flatMap(lambda x: [(tup[0], (x[0],tup[1])) for tup in x[1]] ) \n",
        "    tf_idf = tf.flatMap(lambda x: [(x[0],(tup[0], calc_tf_idf(tup[1], new_index.N , new_index.df[tup[0]], doc_lens[x[0]]))) for tup in x[1]] ) \n",
        "    tf_idf = tf_idf.groupByKey().mapValues(dict)\n",
        "    #truning tf_idf rdd to docid: ([],sum)\n",
        "    tf_idf = tf_idf.mapValues(lambda x: (x,builtins.sum(math.pow(y,2) for y in x.values() )))   \n",
        "    new_index.weights = tf_idf.collectAsMap()\n",
        "    #print(new_index.weights)\n",
        "    #print(new_index.tf_idf)\n",
        "    # write the global stats out\n",
        "    new_index.write_index('.', new_index_name)\n",
        "\n",
        "    return new_index\n"
      ],
      "metadata": {
        "id": "KtTWY040yzJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def create_inverted_index_with_binary_weights(data,new_index_name):\n",
        "\n",
        "    # word counts map - posting lists\n",
        "    word_counts = data.flatMap(lambda x: word_count(x[1], x[0]))\n",
        "    #united posting lists\n",
        "    postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n",
        "    #RDD to map doc_id to doc_len\n",
        "    #doc_lens = data.map(lambda x: (x[0], calc_doc_len(x[1], x[0])))\n",
        "    # turn the doc_lens RDD into dict - TODO: check how its gonna be for the whole corpus\n",
        "    #doc_lens = doc_lens.collectAsMap()\n",
        "    # print(doc_lens)\n",
        "    # RDD to map {doc_id,[(token,tf),(token,tf)....]}\n",
        "    tf = postings.flatMap(lambda x: [(tup[0], (x[0],tup[1])) for tup in x[1]] ) \n",
        "    # groupBy doc_id\n",
        "    tf = tf.groupByKey().mapValues(list)\n",
        "   \n",
        "    # filtering postings and calculate df\n",
        "    #Next, we will filter out rare words, words that appear in 10 or fewer documents (when working on the entire corpus, we will increase this threshold to a minimum of 50 documents).\n",
        "    #postings_filtered = postings.filter(lambda x: len(x[1])>50) #why filter?\n",
        "    #w2df = calculate_df(postings_filtered)\n",
        "    \n",
        "    w2df = calculate_df(postings)\n",
        "    w2df_dict = w2df.collectAsMap()\n",
        "\n",
        "    # partition posting lists and write out- in gcp\n",
        "    #_ = partition_postings_and_write(postings_filtered).collect()\n",
        "    #_ = partition_postings_and_write(postings_filtered)\n",
        "\n",
        "    #in colab:\n",
        "    #write posting lists to disk and returns posting locs dict for each bucket\n",
        "    posting_locs_list = partition_postings_and_write(postings,new_index_name).collect()\n",
        "    \n",
        "\n",
        "    # collect all posting lists locations into one super-set- gcp code:\n",
        "    # super_posting_locs = defaultdict(list)\n",
        "    # for blob in client.list_blobs(bucket_name, prefix='postings_gcp'):\n",
        "    #   if not blob.name.endswith(\"pickle\"):\n",
        "    #     continue\n",
        "    #   with blob.open(\"rb\") as f:\n",
        "    #     posting_locs = pickle.load(f)\n",
        "    #     for k, v in posting_locs.items():\n",
        "    #       super_posting_locs[k].extend(v)\n",
        "\n",
        "    # merge the posting locations into a single dict- good for local test in colab\n",
        "    super_posting_locs = defaultdict(list)\n",
        "    for posting_loc in posting_locs_list:\n",
        "        for k, v in posting_loc.items():\n",
        "          super_posting_locs[k].extend(v)\n",
        "\n",
        "      #for k, v in posting_loc.items():\n",
        "        #super_posting_locs[k].extend(v)\n",
        "\n",
        "    new_index = InvertedIndex()\n",
        "    # Adding the posting locations dictionary to the inverted index\n",
        "    new_index.posting_locs = super_posting_locs\n",
        "    # Add the token - df dictionary to the inverted index\n",
        "    new_index.df = w2df_dict\n",
        "    # the number of the docs in the corpus - using to calc TF-IDF\n",
        "    new_index.N= data.count()\n",
        "    # print(new_index.N)\n",
        "    # tf = postings.flatMap(lambda x: [(tup[0], (x[0],tup[1])) for tup in x[1]] ) \n",
        "    tf_idf = tf.flatMap(lambda x: [(x[0],(tup[0], 1)) for tup in x[1]] ) \n",
        "    tf_idf = tf_idf.groupByKey().mapValues(dict)\n",
        "    tf_idf = tf_idf.mapValues(lambda x: (x,builtins.sum(math.pow(y,2) for y in x.values() )))\n",
        "    new_index.weights = tf_idf.collectAsMap()\n",
        "    #print(new_index.weights)\n",
        "    # write the global stats out\n",
        "    new_index.write_index('.', new_index_name)\n",
        "\n",
        "    return new_index"
      ],
      "metadata": {
        "id": "mdcRcNhkNYO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#make sure that the query is a string with + between the tokens\n",
        "def process_query(query):\n",
        "  tokens = [token.group() for token in RE_WORD.finditer(query.lower())]\n",
        "  tokens = [token for token in tokens if token not in all_stopwords ]\n",
        "  return tokens\n",
        "\n",
        "def get_candidate_doc(processed_query,index):\n",
        "  #returns the mone of cosSim as {docid:mone}\n",
        "  candidate_docs_id={}\n",
        "  #words,pls = zip(*index.posting_lists_iter())\n",
        "  #print(words)\n",
        "  #print(pls)\n",
        "  for term in np.unique(processed_query):\n",
        "    #if term in words:\n",
        "    try:\n",
        "      posting_list = read_posting_list(index,term)\n",
        "      for docid,tf in posting_list:\n",
        "        #dict of {term->weight}\n",
        "        weights = index.weights[docid][0]\n",
        "        candidate_docs_id[docid] = candidate_docs_id.get(docid,0) + weights[term]\n",
        "    except(KeyError): #term is not in the corpus\n",
        "      continue\n",
        "  return candidate_docs_id #if there are no words at all returns none!\n",
        "\n",
        "\n",
        "def calc_cosSim(query,candidate_docs,index):\n",
        "    scores={}\n",
        "    \n"
      ],
      "metadata": {
        "id": "l2UHU3Pi-fbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#creating body index:\n",
        "# time the index creation time\n",
        "#t_start = time()\n",
        "#index_const_time = time() - t_start\n",
        "\n",
        "#creating indexes- one for body, one for title and one for anchor text\n",
        "#body_index = create_inverted_index_with_tf_idf_weights(body_data,\"body_index\")\n",
        "#title_index = create_inverted_index_with_binary_weights(title_data,\"title_index\")\n",
        "title_index = create_inverted_index_with_tf_idf_weights(title_data,\"title_index\")\n",
        "\n",
        "\n",
        "#get_candidate_doc([\"djcfsk\"],title_index)\n",
        "#anchor_index =  create_inverted_index_with_binary_weights(anchor_data,\"anchor_index\")\n",
        "    #docid len map\n",
        "    #doc_len_map = data.Map(lambda x: calc_doc_len(x[1],x[0]))\n",
        "    #print(doc_len_map)\n",
        "\n"
      ],
      "metadata": {
        "id": "5TkL6rxkh4hv",
        "outputId": "dc92b047-ee4f-4adf-a42e-ff1a817202ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{23862: ({'python': 0.3662040962227032, 'programming': 0.3662040962227032, 'language': 0.3662040962227032}, 0.4023163202708606), 7089: ({'chocolate': 1.0986122886681098}, 1.206948960812582), 21035: ({'migraine': 1.0986122886681098}, 1.206948960812582)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(get_candidate_doc([\"python\",\"chocolate\"],title_index))"
      ],
      "metadata": {
        "id": "gstknUmfbiwK",
        "outputId": "54540eb6-a874-4e5a-bf38-953ea1ac27e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{7089: 1.0986122886681098, 23862: 0.3662040962227032}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tests for us- to see that read index works, should also work on full curpos?\n",
        "test_body= InvertedIndex.read_index('.', \"body_index\")\n",
        "test_title = InvertedIndex.read_index('.', \"title_index\")\n",
        "test_anchor = InvertedIndex.read_index('.', \"anchor_index\")\n",
        "print(test_body.weights)\n",
        "print(test_title.weights)\n",
        "print(test_anchor.weights)\n",
        "\n"
      ],
      "metadata": {
        "id": "lhTDLQlaxnpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#posting list by word- could be useful when doing calculation dynamically in relation to the words in the query\n",
        "TUPLE_SIZE = 6       \n",
        "TF_MASK = 2 ** 16 - 1 # Masking the 16 low bits of an integer\n",
        "from contextlib import closing\n",
        "\n",
        "def read_posting_list(inverted, w):\n",
        "  with closing(MultiFileReader()) as reader:\n",
        "    locs = inverted.posting_locs[w]\n",
        "    b = reader.read(locs, inverted.df[w] * TUPLE_SIZE)\n",
        "    posting_list = []\n",
        "    for i in range(inverted.df[w]):\n",
        "      doc_id = int.from_bytes(b[i*TUPLE_SIZE:i*TUPLE_SIZE+4], 'big')\n",
        "      tf = int.from_bytes(b[i*TUPLE_SIZE+4:(i+1)*TUPLE_SIZE], 'big')\n",
        "      posting_list.append((doc_id, tf))\n",
        "    return posting_list"
      ],
      "metadata": {
        "id": "TEVuewd9S-KU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}