{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "testing_and_evaluations.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UT2u3cDcFHgZ"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Aul8VceFLoK",
        "outputId": "7d0c2d98-fc55-4d20-9b84-c46baaf8cce1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "jlHjoANwFMES"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.core.display import Math\n",
        "from inverted_index_gcp import InvertedIndex\n",
        "from inverted_index_gcp import MultiFileReader\n",
        "from inverted_index_gcp import MultiFileWriter\n",
        "import math\n",
        "import numpy as np\n",
        "import builtins #probably won't need this once we have our indexes\n",
        "import nltk\n",
        "from nltk.stem.porter import *\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from collections import OrderedDict, Counter, defaultdict\n",
        "import hashlib\n",
        "from heapq import heappop, heappush, heapify #we use it for searching\n",
        "from time import time #we use it for testing\n",
        "from contextlib import closing #we use it for read posting list\n",
        "import pickle"
      ],
      "metadata": {
        "id": "NcmI4-hQFoiy"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "reading titles_dict"
      ],
      "metadata": {
        "id": "6mszFchGFcND"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/IR_Project/final_indexes/postings_gcp/titles_dict.pkl', 'rb') as f:\n",
        "  titles_dict = pickle.loads(f.read())"
      ],
      "metadata": {
        "id": "IXRMw0mqFeK2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "reading page rank"
      ],
      "metadata": {
        "id": "EczsfJlaF1im"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/IR_Project/final_indexes/postings_gcp/pr.pkl', 'rb') as f:\n",
        "  pr = pickle.loads(f.read())"
      ],
      "metadata": {
        "id": "f-avPTIoFrw3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "reading page views"
      ],
      "metadata": {
        "id": "iYvOKg7bF01d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/IR_Project/final_indexes/postings_gcp/pv.pkl', 'rb') as f:\n",
        "  pv = pickle.loads(f.read())"
      ],
      "metadata": {
        "id": "kX1as73LF-ID"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "reading indexes"
      ],
      "metadata": {
        "id": "KniyAGoAHOAM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "title_index = InvertedIndex.read_index('/content/drive/MyDrive/IR_Project/final_indexes/postings_gcp/',\"title_index\")\n",
        "body_index = InvertedIndex.read_index('/content/drive/MyDrive/IR_Project/final_indexes/postings_gcp/',\"body_index\")\n",
        "anchor_index = InvertedIndex.read_index('/content/drive/MyDrive/IR_Project/final_indexes/postings_gcp/',\"anchor_index\")\n"
      ],
      "metadata": {
        "id": "Efm2ciQlGHdt"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "testing"
      ],
      "metadata": {
        "id": "qqUwU_K9GEux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def intersection(l1,l2):      \n",
        "    \"\"\"\n",
        "    This function perform an intersection between two lists.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    l1: list of documents. Each element is a doc_id.\n",
        "    l2: list of documents. Each element is a doc_id.\n",
        "\n",
        "    Returns:\n",
        "    ----------\n",
        "    list with the intersection (without duplicates) of l1 and l2\n",
        "    \"\"\"\n",
        "    return list(set(l1)&set(l2))\n",
        "\n",
        "#remember to copy the avarege percsision from run _frontend_in_colab\n",
        "def average_precision(true_list, predicted_list, k=40):\n",
        "    #true_set = frozenset(true_list)\n",
        "    true_set = builtins.frozenset(true_list)\n",
        "    predicted_list = predicted_list[:k]\n",
        "    precisions = []\n",
        "    #for i,doc_id in enumerate(predicted_list):        \n",
        "    for i,doc_id in builtins.enumerate(predicted_list):\n",
        "        if doc_id in true_set:\n",
        "            prec = (len(precisions)+1) / (i+1)           \n",
        "            precisions.append(prec)\n",
        "    if len(precisions) == 0:\n",
        "        return 0.0\n",
        "    #return round(sum(precisions)/len(precisions),3)\n",
        "    return builtins.round(builtins.sum(precisions)/len(precisions),3)\n",
        "\n",
        "\n",
        "def precision_at_k(true_list,predicted_list,k=40):    \n",
        "    \"\"\"\n",
        "    This function calculate the precision@k metric.\n",
        "\n",
        "    Parameters\n",
        "    -----------\n",
        "    true_list: list of relevant documents. Each element is a doc_id.\n",
        "    predicted_list: sorted list of documents predicted as relevant. Each element is a doc_id. Sorted is performed by relevance score\n",
        "    k: integer, a number to slice the length of the predicted_list\n",
        "    \n",
        "    Returns:\n",
        "    -----------\n",
        "    float, precision@k with 3 digits after the decimal point.\n",
        "    \"\"\"      \n",
        "    if (k > len(predicted_list)):\n",
        "        k = len(predicted_list)\n",
        "    denominator = k\n",
        "    if k == 0:\n",
        "      return 0\n",
        "    numerator = len(intersection(predicted_list[:k],true_list))\n",
        "    #return round(numerator / denominator,3)\n",
        "    return builtins.round(numerator / denominator,3)\n",
        "\n",
        "def recall_at_k(true_list,predicted_list,k=40):\n",
        "    \"\"\"\n",
        "    This function calculate the recall@k metric.\n",
        "\n",
        "    Parameters\n",
        "    -----------\n",
        "    true_list: list of relevant documents. Each element is a doc_id.\n",
        "    predicted_list: sorted list of documents predicted as relevant. Each element is a doc_id. Sorted is performed by relevance score\n",
        "    k: integer, a number to slice the length of the predicted_list\n",
        "    \n",
        "    Returns:\n",
        "    -----------\n",
        "    float, recall@k with 3 digits after the decimal point.\n",
        "    \"\"\"      \n",
        "    if (k < len(predicted_list)):\n",
        "        predicted_list = predicted_list[:k]\n",
        "    denominator = len(true_list)\n",
        "    if (denominator  == 0):\n",
        "      return 0.0\n",
        "    numerator = len(intersection(predicted_list,true_list))\n",
        "    #return round(numerator / denominator,3)\n",
        "    return builtins.round(numerator / denominator,3)\n",
        "\n",
        "def f_score(true_list,predicted_list,k=40):\n",
        "    \"\"\"\n",
        "    This function calculate the f_score@k metric.\n",
        "\n",
        "    Parameters\n",
        "    -----------\n",
        "    true_list: list of relevant documents. Each element is a doc_id.\n",
        "    predicted_list: sorted list of documents predicted as relevant. Each element is a doc_id. Sorted is performed by relevance score\n",
        "    k: integer, a number to slice the length of the predicted_list\n",
        "    \n",
        "    Returns:\n",
        "    -----------\n",
        "    float, f-score@k with 3 digits after the decimal point.\n",
        "    \"\"\"   \n",
        "    P =  precision_at_k(true_list,predicted_list,k)\n",
        "    R = recall_at_k(true_list,predicted_list,k)\n",
        "    if ( P == 0 or R == 0):\n",
        "      return 0.000\n",
        "    denominator = P + R\n",
        "    numerator = 2 * P * R \n",
        "    #return round(numerator / denominator,3)\n",
        "    return builtins.round(numerator / denominator,3)\n",
        "\n",
        "\n",
        "###need to change it to our terms\n",
        "def evaluate(true_relevancy,predicted_relevancy,k,print_scores=True):\n",
        "    \"\"\"\n",
        "    This function calculates multiple metrics and returns a dictionary with metrics scores across different queries.\n",
        "    Parameters\n",
        "    -----------\n",
        "    true_relevancy: list of tuples indicating the relevancy score for a query. Each element corresponds to a query.\n",
        "    Example of a single element in the list: turn it to [(query as string,true list),()]\n",
        "                                            (3, {'question': ' what problems of heat conduction in composite slabs have been solved so far . ',\n",
        "                                            'relevance_assessments': [(5), (6), (90), (91), (119), (144), (181), (399), (485)]})\n",
        "     \n",
        "    predicted_relevancy: a dictionary of the list. Each key represents the query_id. The value of the dictionary is a sorted list of relevant documents and their scores.\n",
        "                         The list is sorted by the score.  \n",
        "    Example:\n",
        "            key: 1\n",
        "            value: [(13, 17.256625), (486, 13.539465), (12, 9.957595), (746, 9.599499999999999), (51, 9.171265), .....]            \n",
        "            \n",
        "    k: integer, a number to slice the length of the predicted_list\n",
        "    \n",
        "    print_scores: boolean, enable/disable a print of the mean value of each metric.\n",
        "    \n",
        "    Returns:\n",
        "    -----------\n",
        "    a dictionary of metrics scores as follows: \n",
        "                                                        key: metric name\n",
        "                                                        value: list of metric scores. Each element corresponds to a given query.\n",
        "    \"\"\"    \n",
        "    recall_lst = []\n",
        "    precision_lst = []   \n",
        "    avg_precision_lst = []\n",
        "    f_score_lst = []\n",
        "    metrices = {'recall@k':recall_lst,\n",
        "                'precision@k':precision_lst,\n",
        "                'f_score@k': f_score_lst,\n",
        "                'MAP@k':avg_precision_lst}\n",
        "    \n",
        "    for query_id, query_info in true_relevancy:\n",
        "        predicted = predicted_relevancy[query_id]\n",
        "        ground_true = [int(doc_id) for doc_id in query_info['relevance_assessments']]\n",
        "    \n",
        "        recall_lst.append(recall_at_k(ground_true,predicted,k=k))\n",
        "        precision_lst.append(precision_at_k(ground_true,predicted,k=k))\n",
        "        f_score_lst.append(f_score(ground_true,predicted,k=k))\n",
        "        avg_precision_lst.append(average_precision(ground_true,predicted,k=k))\n",
        "\n",
        "    if print_scores:\n",
        "        for name,values in metrices.items():\n",
        "                print(name,sum(values)/len(values))\n",
        "\n",
        "    return metrices    "
      ],
      "metadata": {
        "id": "2jn4K3ChGFpc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}