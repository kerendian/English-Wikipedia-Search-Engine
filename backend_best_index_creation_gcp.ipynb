{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"K_DmtmTKGfW8"},"outputs":[{"name":"stdout","output_type":"stream","text":["NAME          PLATFORM  WORKER_COUNT  PREEMPTIBLE_WORKER_COUNT  STATUS   ZONE           SCHEDULED_DELETE\r\n","cluster-cfd6  GCE       5                                       RUNNING  us-central1-a\r\n"]}],"source":["!gcloud dataproc clusters list --region us-central1"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"0aGqaAjgGh9L"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\n","\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\n"]}],"source":["!pip install -q google-cloud-storage==1.43.0\n","!pip install -q graphframes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X2QriP4vGlmJ"},"outputs":[],"source":["import pyspark\n","import sys\n","from collections import Counter, OrderedDict, defaultdict\n","import itertools\n","from itertools import islice, count, groupby\n","import pandas as pd\n","import os\n","import re\n","from operator import itemgetter\n","import nltk\n","from nltk.stem.porter import *\n","from nltk.corpus import stopwords\n","from time import time\n","from pathlib import Path\n","import pickle\n","import pandas as pd\n","from google.cloud import storage\n","import math\n","import numpy as np\n","import builtins\n","from contextlib import closing\n","import hashlib\n","from time import time #we use it for testing\n","\n","def _hash(s):\n","    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n","\n","nltk.download('stopwords')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OIxXK1kpGnwX"},"outputs":[],"source":["!ls -l /usr/lib/spark/jars/graph*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ijTLGPKiGpXk"},"outputs":[],"source":["from pyspark.sql import *\n","from pyspark.sql.functions import *\n","from pyspark import SparkContext, SparkConf, SparkFiles\n","from pyspark.sql import SQLContext\n","from graphframes import *"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NdSlpl98GtVZ"},"outputs":[],"source":["bucket_name = '3b1u8c3k6e4t122' \n","client = storage.Client()\n","blobs = client.list_blobs(bucket_name)\n","for b in blobs:\n","    print(b.name)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_s0Hdv_RGt9g"},"outputs":[],"source":["# if nothing prints here you forgot to upload the file inverted_index_gcp.py to the home dir\n","%cd -q /home/dataproc\n","!ls inverted_index_gcp.py"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nqfs30_HGxO0"},"outputs":[],"source":["# adding our python module to the cluster\n","sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n","sys.path.insert(0,SparkFiles.getRootDirectory())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SgUswxDBGyw3"},"outputs":[],"source":["from inverted_index_gcp import InvertedIndex\n","from inverted_index_gcp import MultiFileReader\n","from inverted_index_gcp import MultiFileWriter\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y3dJZAy8G0D3"},"outputs":[],"source":["#rdds of which we build the different indexes on\n","body_data = parquetFile.select(\"id\",\"text\").rdd\n","title_data = parquetFile.select(\"id\",\"title\").rdd\n","anchor_data = parquetFile.select(\"id\",\"anchor_text\").rdd\n","anchor_data= anchor_data.flatMap(lambda l: [(value[0], value[1]) for value in l[1]])  \n","anchor_data= anchor_data.groupByKey().mapValues(list)\n","anchor_data=anchor_data.mapValues(lambda x: ','.join(set(item for item in x)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CERB0Af5G5ia"},"outputs":[],"source":["#using code from assignment 3- gcp\n","#funcs to help build the inverted index\n","english_stopwords = frozenset(stopwords.words('english'))\n","corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\", \n","                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\", \n","                    \"part\", \"thumb\", \"including\", \"second\", \"following\", \n","                    \"many\", \"however\", \"would\", \"became\", \"redirect\",\"user\"]\n","\n","all_stopwords = english_stopwords.union(corpus_stopwords)\n","RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n","\n","NUM_BUCKETS = 124\n","def token2bucket_id(token):\n","  return int(_hash(token),16) % NUM_BUCKETS\n","\n","def calc_doc_len(text, id):\n","  tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n","  tokens = [token for token in tokens if token not in all_stopwords]\n","  \n","  return len(tokens)\n","\n","def word_count(text, id):\n","  \n","  ''' Count the frequency of each word in `text` (tf) that is not included in \n","  `all_stopwords` and return entries that will go into our posting lists. \n","  Parameters:\n","  -----------\n","    text: str\n","      Text of one document\n","    id: int\n","      Document id\n","  Returns:\n","  --------\n","    List of tuples\n","      A list of (token, (doc_id, tf)) pairs \n","      for example: [(\"Anarchism\", (12, 5)), ...]\n","  '''\n","  tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n","  tokens = [token for token in tokens if token not in all_stopwords ]\n","  word_count_list = []\n","  words_dict= OrderedDict()\n","  for word in tokens:\n","    words_dict[word] = words_dict.get(word,0) + 1\n","  for item in words_dict.items():\n","    word_count_list.append((item[0],(id,item[1])))\n","  return word_count_list\n","\n","def reduce_word_counts(unsorted_pl):\n","  ''' Returns a sorted posting list by wiki_id.\n","  Parameters:\n","  -----------\n","    unsorted_pl: list of tuples\n","      A list of (wiki_id, tf) tuples \n","  Returns:\n","  --------\n","    list of tuples\n","      A sorted posting list.\n","  '''\n","  return  sorted(unsorted_pl,key=lambda y: y[0])\n","\n","def calculate_df(postings):\n","  ''' Takes a posting list RDD and calculate the df for each token.\n","  Parameters:\n","  -----------\n","    postings: RDD\n","      An RDD where each element is a (token, posting_list) pair.\n","  Returns:\n","  --------\n","    RDD\n","      An RDD where each element is a (token, df) pair.\n","  '''\n","\n","  return postings.mapValues(lambda x : len(x))\n","\n","def partition_postings_and_write(postings,new_index_name):\n","  ''' A function that partitions the posting lists into buckets, writes out \n","  all posting lists in a bucket to disk, and returns the posting locations for \n","  each bucket. Partitioning should be done through the use of `token2bucket` \n","  above. Writing to disk should use the function  `write_a_posting_list`, a \n","  static method implemented in inverted_index_colab.py under the InvertedIndex \n","  class. \n","  Parameters:\n","  -----------\n","    postings: RDD\n","      An RDD where each item is a (w, posting_list) pair.\n","  Returns:\n","  --------\n","    RDD\n","      An RDD where each item is a posting locations dictionary for a bucket. The\n","      posting locations maintain a list for each word of file locations and \n","      offsets its posting list was written to. See `write_a_posting_list` for \n","      more details.\n","  '''\n","  #partitioning RDD to (bucket_id, [(w0, posting_list_0)) entries \n","  postings = postings.map(lambda x: (new_index_name+str(token2bucket_id(x[0])),x))\n","  #reducing it to (bucket_id, [(w0, posting_list_0), (w1, posting_list_1), ...])\n","  postings = postings.groupByKey()\n","  #writing each bucket to disk\n","  #need to change it in gcp to:\n","  postings = postings.map(lambda y: (InvertedIndex.write_a_posting_list(y,'3b1u8c3k6e4t122')))\n","  return postings\n","  \n","def calc_bm25(doc_len, avgdl,k,b):\n","  \n","  return k * (1 - b + b * (doc_len/avgdl))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1WH3guhDG9QW"},"outputs":[],"source":["def create_body_index_with_bm_25_weights(data,new_index_name):\n","    # word counts map - posting lists\n","    word_counts = data.flatMap(lambda x: word_count(x[1], x[0]))\n","    #united posting lists\n","    postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n","    #Next, we will filter out rare words, words that appear in 10 or fewer documents (when working on the entire corpus, we will increase this threshold to a minimum of 50 documents).\n","    #in gcp: change filter to 50\n","    postings = postings.filter(lambda x: len(x[1])>50) \n","    # RDD to map {doc_id,[(token,tf),(token,tf)....]}\n","    tf = postings.flatMap(lambda x: [(tup[0], (x[0],tup[1])) for tup in x[1]] ) \n","    # groupBy doc_id\n","    tf = tf.groupByKey().mapValues(list)\n","    #RDD to map doc_id to doc_len\n","    doc_lens_rdd = tf.map(lambda x: (x[0],builtins.sum([tup[1] for tup in x[1]])))\n","    # turn the doc_lens RDD into dict - TODO: check how its gonna be for the whole corpus\n","    doc_lens = doc_lens_rdd.collectAsMap()\n","    # filtering postings and calculate df\n","    w2df = calculate_df(postings)\n","    w2df_dict = w2df.collectAsMap()\n","\n","    avgdl = builtins.sum(doc_lens.values())/len(doc_lens)\n","    #write posting lists to disk and returns posting locs dict for each bucket\n","    posting_locs_list = partition_postings_and_write(postings,new_index_name).collect()\n","    \n","\n","    # collect all posting lists locations into one super-set- gcp code:\n","    super_posting_locs = defaultdict(list)\n","    for blob in client.list_blobs(bucket_name, prefix='postings_gcp'):\n","      if not blob.name.endswith(\"pickle\"):\n","        continue\n","      elif not blob.name.startswith(f'postings_gcp/{new_index_name}'):\n","        continue\n","      with blob.open(\"rb\") as f:\n","        posting_locs = pickle.load(f)\n","        for k, v in posting_locs.items():\n","          super_posting_locs[k].extend(v)\n","\n","    # merge the posting locations into a single dict- good for local test in colab\n","#     super_posting_locs = defaultdict(list)\n","#     for posting_loc in posting_locs_list:\n","#         for k, v in posting_loc.items():\n","#           super_posting_locs[k].extend(v)\n","\n","    new_index = InvertedIndex()\n","    # Adding the posting locations dictionary to the inverted index\n","    new_index.posting_locs = super_posting_locs\n","    # Add the token - df dictionary to the inverted index\n","    new_index.df = w2df_dict\n","    # the number of the docs in the corpus - using to calc TF-IDF\n","    new_index.N= data.count()\n","    #{doc_id:bm25_mechane}\n","    bm25 = doc_lens_rdd.map(lambda x: (x[0],calc_bm25(x[1], avgdl,1.5,0.75)))\n","    \n","    new_index.weights = bm25.collectAsMap()\n","    # write the global stats out\n","    new_index.write_index('.', new_index_name)\n","\n","    return new_index"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lpwE-KWJWfVt"},"outputs":[],"source":["def create_title_anchor_index_with_bm_25_weights(data,new_index_name):\n","    # word counts map - posting lists\n","    word_counts = data.flatMap(lambda x: word_count(x[1], x[0]))\n","    #united posting lists\n","    postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n","    #Next, we will filter out rare words, words that appear in 10 or fewer documents (when working on the entire corpus, we will increase this threshold to a minimum of 50 documents).\n","    #in gcp: change filter to 50\n","    #postings = postings.filter(lambda x: len(x[1])>50) \n","    # RDD to map {doc_id,[(token,tf),(token,tf)....]}\n","    tf = postings.flatMap(lambda x: [(tup[0], (x[0],tup[1])) for tup in x[1]] ) \n","    # groupBy doc_id\n","    tf = tf.groupByKey().mapValues(list)\n","    #RDD to map doc_id to doc_len\n","    doc_lens_rdd = tf.map(lambda x: (x[0],builtins.sum([tup[1] for tup in x[1]])))\n","    # turn the doc_lens RDD into dict - TODO: check how its gonna be for the whole corpus\n","    doc_lens = doc_lens_rdd.collectAsMap()\n","    # filtering postings and calculate df\n","    w2df = calculate_df(postings)\n","    w2df_dict = w2df.collectAsMap()\n","\n","    avgdl = builtins.sum(doc_lens.values())/len(doc_lens)\n","    #write posting lists to disk and returns posting locs dict for each bucket\n","    posting_locs_list = partition_postings_and_write(postings,new_index_name).collect()\n","    \n","\n","    # collect all posting lists locations into one super-set- gcp code:\n","    super_posting_locs = defaultdict(list)\n","    for blob in client.list_blobs(bucket_name, prefix='postings_gcp'):\n","      if not blob.name.endswith(\"pickle\"):\n","        continue\n","      elif not blob.name.startswith(f'postings_gcp/{new_index_name}'):\n","        continue\n","      with blob.open(\"rb\") as f:\n","        posting_locs = pickle.load(f)\n","        for k, v in posting_locs.items():\n","          super_posting_locs[k].extend(v)\n","\n","    # merge the posting locations into a single dict- good for local test in colab\n","#     super_posting_locs = defaultdict(list)\n","#     for posting_loc in posting_locs_list:\n","#         for k, v in posting_loc.items():\n","#           super_posting_locs[k].extend(v)\n","\n","    new_index = InvertedIndex()\n","    # Adding the posting locations dictionary to the inverted index\n","    new_index.posting_locs = super_posting_locs\n","    # Add the token - df dictionary to the inverted index\n","    new_index.df = w2df_dict\n","    # the number of the docs in the corpus - using to calc TF-IDF\n","    new_index.N= data.count()\n","    #{doc_id:bm25_mechane}\n","    bm25 = doc_lens_rdd.map(lambda x: (x[0],calc_bm25(x[1], avgdl,1.5,0.75)))\n","    \n","    new_index.weights = bm25.collectAsMap()\n","    # write the global stats out\n","    new_index.write_index('.', new_index_name)\n","\n","    return new_index"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["bm25_B= create_body_index_with_bm_25_weights(body_data,\"bm25_B\")\n","print(\"done creating body\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["index_src = \"bm25_B.pkl\"\n","index_dst = f'gs://{bucket_name}/postings_gcp/{index_src}'"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["bm25_T= create_title_anchor_index_with_bm_25_weights(title_data,\"bm25_T\")\n","print(\"done creating title\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["index_src = \"bm25_T.pkl\"\n","index_dst = f'gs://{bucket_name}/postings_gcp/{index_src}'"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["bm25_A= create_title_anchor_index_with_bm_25_weights(anchor_data,\"bm25_A\")\n","print(\"done creating anchor\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["index_src = \"bm25_A.pkl\"\n","index_dst = f'gs://{bucket_name}/postings_gcp/{index_src}'"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"backend_best_index_creation_gcp.ipynb","provenance":[]},"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"}},"nbformat":4,"nbformat_minor":1}